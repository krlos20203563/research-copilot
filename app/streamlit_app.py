"""
streamlit_app.py
--------
Research Copilot â€” RAG interface for 20 academic papers on criminal governance
and extortion in Latin America.

The app requires an OpenAI API key entered at runtime. The key is stored only
in st.session_state (memory) and is never written to disk or source code.

Run with:
    streamlit run app/streamlit_app.py
"""
from __future__ import annotations

import json
import os
import sys
from pathlib import Path

import streamlit as st

# Make project root importable
ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT))

# â”€â”€ Page config (must be first Streamlit call) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Research Copilot â€” Crimen Organizado & Gobernanza Criminal",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ Session state defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "api_key" not in st.session_state:
    st.session_state.api_key = ""
if "api_key_validated" not in st.session_state:
    st.session_state.api_key_validated = False
if "messages" not in st.session_state:
    st.session_state.messages = []


# â”€â”€ API Key gate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _validate_api_key(key: str) -> tuple[bool, str]:
    """
    Do a lightweight check: verify the key is non-empty and well-formed,
    then make a minimal API call to confirm it works.
    Returns (is_valid, error_message).
    """
    if not key or not key.strip():
        return False, "La clave no puede estar vacÃ­a."
    key = key.strip()
    if not key.startswith("sk-"):
        return False, "Una API key de OpenAI debe comenzar con 'sk-'."
    try:
        from openai import OpenAI, AuthenticationError
        client = OpenAI(api_key=key)
        # Minimal call â€” list models is cheap and fast
        client.models.list()
        return True, ""
    except AuthenticationError:
        return False, "API key invÃ¡lida. Verifica que la clave es correcta."
    except Exception as exc:
        return False, f"Error al verificar la clave: {exc}"


def render_api_key_gate() -> bool:
    """
    Show the full-page API key input screen.
    Returns True only when a valid key is stored in session state.
    """
    if st.session_state.api_key_validated:
        return True

    # Center the form
    _, col, _ = st.columns([1, 2, 1])
    with col:
        st.markdown("## ğŸ“š Research Copilot")
        st.markdown(
            "Asistente de investigaciÃ³n para 20 artÃ­culos acadÃ©micos sobre "
            "**crimen organizado, extorsiÃ³n y gobernanza criminal** en AmÃ©rica Latina."
        )
        st.divider()
        st.markdown("### ğŸ”‘ Ingresa tu OpenAI API Key")
        st.caption(
            "La clave se almacena Ãºnicamente en memoria durante esta sesiÃ³n "
            "y nunca se guarda en disco ni en el cÃ³digo fuente. "
            "ObtÃ©n la tuya en [platform.openai.com/api-keys](https://platform.openai.com/api-keys)."
        )

        with st.form("api_key_form", clear_on_submit=False):
            key_input = st.text_input(
                "API Key",
                type="password",
                placeholder="sk-...",
                label_visibility="collapsed",
            )
            submitted = st.form_submit_button(
                "Iniciar Research Copilot â†’",
                type="primary",
                use_container_width=True,
            )

        if submitted:
            with st.spinner("Verificando claveâ€¦"):
                valid, error_msg = _validate_api_key(key_input)

            if valid:
                st.session_state.api_key = key_input.strip()
                st.session_state.api_key_validated = True
                st.rerun()
            else:
                st.error(f"âŒ {error_msg}")

        st.divider()
        st.caption(
            "ğŸ’¡ **Para desarrollo local:** crea un archivo `.env` con "
            "`OPENAI_API_KEY=sk-...` y la app lo cargarÃ¡ automÃ¡ticamente."
        )

    return False


# â”€â”€ Try loading key from .env for local development (never from source code) â”€
def _try_load_env_key():
    """
    Load key from .env if present and not already set.
    This only runs once at startup. The key is stored in session state only.
    """
    if st.session_state.api_key_validated:
        return
    try:
        from dotenv import dotenv_values
        env = dotenv_values(ROOT / ".env")
        key = env.get("OPENAI_API_KEY", "").strip()
        if key and key != "sk-...your-key-here...":
            st.session_state.api_key = key
            st.session_state.api_key_validated = True
    except Exception:
        pass


# â”€â”€ Cached resources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_resource(show_spinner="Conectando con ChromaDBâ€¦")
def get_chroma_client():
    from src.vectorstore import get_chroma_client as _get
    return _get()


@st.cache_data(show_spinner="Cargando metadatos de papersâ€¦")
def load_papers_metadata():
    json_path = ROOT / "papers" / "papers.json"
    if not json_path.exists():
        return []
    with open(json_path, encoding="utf-8") as f:
        data = json.load(f)
    return data.get("papers", [])


def get_openai_client():
    """Return an OpenAI client using the session-state key."""
    from openai import OpenAI
    return OpenAI(api_key=st.session_state.api_key)


# â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_sidebar():
    with st.sidebar:
        st.title("âš™ï¸ ConfiguraciÃ³n")

        strategy = st.selectbox(
            "Estrategia de prompts",
            options=["delimiters", "json", "fewshot", "cot"],
            format_func=lambda x: {
                "delimiters": "1 â€” Delimitadores",
                "json": "2 â€” Salida JSON",
                "fewshot": "3 â€” Few-Shot",
                "cot": "4 â€” Chain-of-Thought",
            }[x],
            key="strategy",
        )

        chunk_strategy = st.radio(
            "TamaÃ±o de chunks",
            options=["small", "large"],
            format_func=lambda x: "256 tokens" if x == "small" else "1024 tokens",
            horizontal=True,
            key="chunk_strategy",
        )

        top_k = st.slider("Top-k fragmentos", 1, 10, 5, key="top_k")

        st.divider()

        # Show masked key indicator + logout
        masked = "sk-â€¦" + st.session_state.api_key[-4:] if len(st.session_state.api_key) > 6 else "â€”"
        st.caption(f"ğŸ”‘ API Key: `{masked}`")
        if st.button("ğŸ”’ Cerrar sesiÃ³n / Cambiar clave", use_container_width=True):
            st.session_state.api_key = ""
            st.session_state.api_key_validated = False
            st.session_state.messages = []
            st.rerun()

        st.divider()
        st.caption("Research Copilot v0.1.0")

    return strategy, chunk_strategy, top_k


# â”€â”€ Tab 1: Chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_chat_tab(strategy: str, chunk_strategy: str, top_k: int):
    st.header("ğŸ’¬ Chat con los Papers")
    st.caption(
        "Haz preguntas sobre crimen organizado, extorsiÃ³n y gobernanza criminal "
        "en AmÃ©rica Latina. Las respuestas se basan en los 20 artÃ­culos indexados."
    )

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg.get("sources"):
                with st.expander("ğŸ“ Fuentes consultadas", expanded=False):
                    for src in msg["sources"]:
                        st.markdown(
                            f"**{src['title']}** â€” {', '.join(src['authors'][:2])} ({src['year']})  \n"
                            f"Relevancia: `{src['score']:.3f}` | {src.get('venue', '')}"
                        )

    if prompt := st.chat_input("Â¿CuÃ¡l es tu pregunta de investigaciÃ³n?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Buscando en los papersâ€¦"):
                answer, sources = _run_rag(prompt, strategy, chunk_strategy, top_k)
            st.markdown(answer)
            if sources:
                with st.expander("ğŸ“ Fuentes consultadas", expanded=True):
                    for src in sources:
                        st.markdown(
                            f"**{src['title']}** â€” {', '.join(src['authors'][:2])} ({src['year']})  \n"
                            f"Relevancia: `{src['score']:.3f}` | {src.get('venue', '')}"
                        )

        st.session_state.messages.append(
            {"role": "assistant", "content": answer, "sources": sources}
        )

    if st.session_state.messages:
        if st.button("ğŸ—‘ï¸ Limpiar conversaciÃ³n", key="clear_chat"):
            st.session_state.messages = []
            st.rerun()


def _run_rag(question: str, strategy: str, chunk_strategy: str, top_k: int):
    from src.retrieval import search
    from src.generation import generate_answer

    chroma = get_chroma_client()
    oa = get_openai_client()

    try:
        chunks = search(
            question,
            top_k=top_k,
            strategy=chunk_strategy,
            chroma_client=chroma,
            openai_client=oa,
        )
    except RuntimeError as e:
        return (
            f"âš ï¸ El Ã­ndice no estÃ¡ construido. Ejecuta primero:\n\n"
            f"```\npython -m src.vectorstore\n```\n\nError: {e}",
            [],
        )

    result = generate_answer(question=question, chunks=chunks, strategy=strategy, client=oa)
    sources = [
        {"title": c.title, "authors": c.authors, "year": c.year,
         "venue": c.venue, "score": c.score}
        for c in chunks
    ]
    return result["answer"], sources


# â”€â”€ Tab 2: Paper Browser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_papers_tab():
    st.header("ğŸ“„ Explorador de Papers")
    papers = load_papers_metadata()
    if not papers:
        st.warning("No se encontrÃ³ papers.json.")
        return

    col1, col2, col3 = st.columns(3)
    with col1:
        years = sorted({p["year"] for p in papers if p.get("year")})
        selected_years = st.multiselect("AÃ±o", options=years)
    with col2:
        all_topics = sorted({t for p in papers for t in (p.get("topics") or [])})
        selected_topics = st.multiselect("Tema", options=all_topics)
    with col3:
        search_text = st.text_input("Buscar por tÃ­tulo / autor", "")

    filtered = papers
    if selected_years:
        filtered = [p for p in filtered if p.get("year") in selected_years]
    if selected_topics:
        filtered = [p for p in filtered if any(t in (p.get("topics") or []) for t in selected_topics)]
    if search_text:
        q = search_text.lower()
        filtered = [
            p for p in filtered
            if q in p.get("title", "").lower()
            or any(q in a.lower() for a in (p.get("authors") or []))
        ]

    st.caption(f"Mostrando {len(filtered)} de {len(papers)} papers")

    for paper in filtered:
        authors_str = "; ".join((paper.get("authors") or [])[:3])
        if len(paper.get("authors") or []) > 3:
            authors_str += " et al."

        with st.expander(
            f"**{paper.get('title', '?')}** â€” {authors_str} ({paper.get('year', '?')})"
        ):
            col_a, col_b = st.columns([2, 1])
            with col_a:
                st.markdown(f"**Autores:** {authors_str}")
                st.markdown(f"**Venue:** {paper.get('venue') or 'â€”'}")
                if paper.get("doi"):
                    st.markdown(f"**DOI:** [{paper['doi']}](https://doi.org/{paper['doi']})")
                if paper.get("abstract"):
                    st.markdown("**Abstract:**")
                    st.caption(paper["abstract"])
            with col_b:
                for t in (paper.get("topics") or []):
                    st.markdown(f"- {t}")


# â”€â”€ Tab 3: Compare Strategies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_compare_tab(chunk_strategy: str, top_k: int):
    st.header("ğŸ”¬ Comparar Estrategias de Prompts")
    st.caption("Compara las 4 estrategias de prompting sobre la misma pregunta.")

    question = st.text_area(
        "Pregunta:",
        value="Â¿CuÃ¡l es la relaciÃ³n entre gobernanza criminal y el Estado en AmÃ©rica Latina?",
        height=80,
        key="compare_question",
    )

    if st.button("â–¶ Ejecutar las 4 estrategias", type="primary", key="run_compare"):
        from src.retrieval import search
        from src.generation import generate_answer, STRATEGY_LABELS

        chroma = get_chroma_client()
        oa = get_openai_client()

        with st.spinner("Recuperando fragmentosâ€¦"):
            try:
                chunks = search(question, top_k=top_k, strategy=chunk_strategy,
                                chroma_client=chroma, openai_client=oa)
            except RuntimeError as e:
                st.error(f"Index not built: {e}")
                return

        st.success(f"Recuperados {len(chunks)} fragmentos.")
        with st.expander("ğŸ“ Fragmentos recuperados", expanded=False):
            for c in chunks:
                st.markdown(f"**[{c.score:.3f}]** {c.title} ({c.year}) â€” chunk {c.chunk_index}")
                st.caption(c.text[:300] + "â€¦")

        cols = st.columns(2)
        for i, strat in enumerate(["delimiters", "json", "fewshot", "cot"]):
            with cols[i % 2]:
                st.subheader(STRATEGY_LABELS[strat])
                with st.spinner(f"Generando con {strat}â€¦"):
                    res = generate_answer(question=question, chunks=chunks,
                                         strategy=strat, client=oa)
                st.markdown(res["answer"])
                st.caption(f"Tokens: {res['total_tokens']} | Tiempo: {res['elapsed_seconds']}s")
                st.divider()


# â”€â”€ Tab 4: About â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_about_tab():
    st.header("â„¹ï¸ Acerca del Research Copilot")
    papers = load_papers_metadata()

    st.markdown("""
## Arquitectura

```
PDF Papers â†’ PyMuPDF â†’ Chunking (256 / 1024 tokens) â†’ text-embedding-3-small
                                                              â†“
                                                       ChromaDB (cosine)
                                                              â†“
User Query â†’ embed â†’ Top-K Retrieval â†’ Prompt Strategy â†’ GPT-4o â†’ Answer
```

## Estrategias de Prompting

| # | Estrategia | DescripciÃ³n |
|---|-----------|-------------|
| 1 | **Delimitadores** | Secciones XML (`<<<CONTEXTO>>>`, `<<<PREGUNTA>>>`) |
| 2 | **JSON Output** | Respuesta estructurada con campos predefinidos |
| 3 | **Few-Shot** | Dos ejemplos trabajados enseÃ±an el estilo esperado |
| 4 | **Chain-of-Thought** | 5 pasos explÃ­citos de razonamiento antes de responder |

## Seguridad

- La API key **nunca** se almacena en el cÃ³digo fuente ni en archivos versionados.
- Se solicita al usuario al inicio de cada sesiÃ³n y se guarda solo en memoria.
- El repositorio es seguro para hacerse pÃºblico en GitHub.

## Papers indexados
""")

    if papers:
        for p in papers:
            authors = (p.get("authors") or [])[:2]
            st.markdown(f"- **{p.get('title', '?')}** â€” {'; '.join(authors)} ({p.get('year', '?')})")

    st.markdown("""
## Uso local

```bash
git clone <repo>
cd Tarea_1
pip install -r requirements.txt

# Indexar papers (una sola vez)
python -m src.vectorstore

# Lanzar la app
streamlit run app/streamlit_app.py
```

La app pedirÃ¡ tu API key en el navegador.
""")


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    # 1. Try loading key from local .env (dev convenience â€” never committed)
    _try_load_env_key()

    # 2. Gate: show API key input if not authenticated
    if not render_api_key_gate():
        st.stop()

    # 3. Main app
    strategy, chunk_strategy, top_k = render_sidebar()

    tab_chat, tab_papers, tab_compare, tab_about = st.tabs([
        "ğŸ’¬ Chat", "ğŸ“„ Papers", "ğŸ”¬ Comparar", "â„¹ï¸ Acerca de",
    ])

    with tab_chat:
        render_chat_tab(strategy, chunk_strategy, top_k)
    with tab_papers:
        render_papers_tab()
    with tab_compare:
        render_compare_tab(chunk_strategy, top_k)
    with tab_about:
        render_about_tab()


if __name__ == "__main__":
    main()
