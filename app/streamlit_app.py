"""
streamlit_app.py
----------------
Research Copilot â€” RAG interface for 20 academic papers on criminal governance
and extortion in Latin America.

Flow:
  1. API Key gate  â€” user enters OpenAI key (stored in session state only)
  2. Index gate    â€” auto-detects ChromaDB; builds it if missing (first run)
  3. Main app      â€” Chat / Papers / Compare / About

Run locally:
    streamlit run app/streamlit_app.py
"""
from __future__ import annotations

import json
import os
import sys
from pathlib import Path

import streamlit as st

ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT))

# â”€â”€ Page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Research Copilot",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ Session state defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for _k, _v in {
    "api_key": "",
    "api_key_validated": False,
    "index_ready": False,
    "messages": [],
}.items():
    if _k not in st.session_state:
        st.session_state[_k] = _v


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 1 â€” API KEY GATE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _validate_api_key(key: str) -> tuple[bool, str]:
    key = key.strip()
    if not key:
        return False, "La clave no puede estar vacÃ­a."
    if not key.startswith("sk-"):
        return False, "Una API key de OpenAI debe comenzar con 'sk-'."
    try:
        from openai import OpenAI, AuthenticationError
        OpenAI(api_key=key).models.list()
        return True, ""
    except AuthenticationError:
        return False, "API key invÃ¡lida. Verifica que sea correcta."
    except Exception as exc:
        return False, f"Error al verificar: {exc}"


def _try_load_env_key():
    """Load key from local .env if present (dev convenience, never committed)."""
    if st.session_state.api_key_validated:
        return
    try:
        from dotenv import dotenv_values
        key = dotenv_values(ROOT / ".env").get("OPENAI_API_KEY", "").strip()
        if key and not key.startswith("sk-..."):
            st.session_state.api_key = key
            st.session_state.api_key_validated = True
    except Exception:
        pass


def render_api_key_gate() -> bool:
    """Returns True if a valid key is in session state."""
    if st.session_state.api_key_validated:
        return True

    _, col, _ = st.columns([1, 2, 1])
    with col:
        st.markdown("## ğŸ“š Research Copilot")
        st.markdown(
            "Asistente de investigaciÃ³n para **20 artÃ­culos acadÃ©micos** sobre "
            "crimen organizado, extorsiÃ³n y gobernanza criminal en AmÃ©rica Latina."
        )
        st.divider()
        st.markdown("### ğŸ”‘ Ingresa tu OpenAI API Key")
        st.caption(
            "La clave se guarda solo en memoria (sesiÃ³n). Nunca se escribe en disco "
            "ni en el cÃ³digo. "
            "ObtÃ©n la tuya en [platform.openai.com/api-keys](https://platform.openai.com/api-keys)."
        )
        with st.form("api_key_form"):
            key_input = st.text_input(
                "API Key", type="password", placeholder="sk-...",
                label_visibility="collapsed",
            )
            if st.form_submit_button("Iniciar Research Copilot â†’", type="primary",
                                     use_container_width=True):
                with st.spinner("Verificandoâ€¦"):
                    ok, msg = _validate_api_key(key_input)
                if ok:
                    st.session_state.api_key = key_input.strip()
                    st.session_state.api_key_validated = True
                    st.rerun()
                else:
                    st.error(f"âŒ {msg}")
        st.divider()
        st.caption("ğŸ’¡ **Desarrollo local:** crea `.env` con `OPENAI_API_KEY=sk-...`")
    return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2 â€” INDEX GATE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _index_exists() -> bool:
    """Check if either ChromaDB collection has at least one document."""
    try:
        from src.vectorstore import (
            CHROMA_PERSIST_DIR, COLLECTION_SMALL,
            get_chroma_client, get_or_create_collection,
        )
        client = get_chroma_client(CHROMA_PERSIST_DIR)
        col = get_or_create_collection(client, COLLECTION_SMALL)
        return col.count() > 0
    except Exception:
        return False


def _build_index(api_key: str, status_placeholder):
    """Build both ChromaDB collections. Streams progress to status_placeholder."""
    from src.ingestion import load_papers
    from src.chunking import chunk_papers
    from src.embedding import embed_texts
    from src.vectorstore import (
        CHROMA_PERSIST_DIR, COLLECTION_SMALL, COLLECTION_LARGE,
        get_chroma_client, get_or_create_collection, index_chunks,
    )
    from openai import OpenAI

    oa = OpenAI(api_key=api_key)
    chroma = get_chroma_client(CHROMA_PERSIST_DIR)

    status_placeholder.info("ğŸ“‚ Leyendo los 20 PDFsâ€¦")
    papers = load_papers(verbose=False)

    for strategy, col_name in [("small", COLLECTION_SMALL), ("large", COLLECTION_LARGE)]:
        label = "256 tokens" if strategy == "small" else "1024 tokens"
        status_placeholder.info(f"âœ‚ï¸ Chunking ({label})â€¦")
        chunks = chunk_papers(papers, strategy=strategy)

        col = get_or_create_collection(chroma, col_name)
        status_placeholder.info(
            f"ğŸ”¢ Generando embeddings para {len(chunks)} chunks ({label})â€¦  \n"
            "Esto toma ~2-3 minutos la primera vez."
        )
        index_chunks(chunks, col, show_progress=False)

    status_placeholder.success("âœ… Ãndice construido. Â¡Listo para usar!")


def render_index_gate() -> bool:
    """
    Returns True if the index is ready.
    On first run (no index), shows a build button.
    """
    if st.session_state.index_ready:
        return True

    if _index_exists():
        st.session_state.index_ready = True
        return True

    # Index missing â€” show setup screen
    _, col, _ = st.columns([1, 2, 1])
    with col:
        st.markdown("## âš™ï¸ Primera configuraciÃ³n")
        st.markdown(
            "El Ã­ndice vectorial no existe todavÃ­a. "
            "Hay que procesar los **20 PDFs** y generar sus embeddings.  \n"
            "Esto ocurre **una sola vez** (~2-3 min) y luego se guarda."
        )
        st.info(
            "ğŸ“Œ Se usarÃ¡ tu API key para llamar a `text-embedding-3-small`.  \n"
            "Costo aproximado: **< $0.05 USD** por indexaciÃ³n completa."
        )
        status = st.empty()
        if st.button("ğŸš€ Construir Ã­ndice ahora", type="primary", use_container_width=True):
            with st.spinner("Construyendo Ã­ndiceâ€¦ no cierres esta pestaÃ±a."):
                try:
                    _build_index(st.session_state.api_key, status)
                    st.session_state.index_ready = True
                    st.rerun()
                except Exception as exc:
                    st.error(f"Error al construir el Ã­ndice: {exc}")
    return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CACHED RESOURCES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@st.cache_resource(show_spinner=False)
def get_chroma_client_cached():
    from src.vectorstore import get_chroma_client, CHROMA_PERSIST_DIR
    return get_chroma_client(CHROMA_PERSIST_DIR)


@st.cache_data(show_spinner=False)
def load_papers_metadata():
    json_path = ROOT / "papers" / "papers.json"
    if not json_path.exists():
        return []
    with open(json_path, encoding="utf-8") as f:
        return json.load(f).get("papers", [])


def get_openai_client():
    from openai import OpenAI
    return OpenAI(api_key=st.session_state.api_key)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SIDEBAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_sidebar():
    with st.sidebar:
        st.title("âš™ï¸ ConfiguraciÃ³n")

        strategy = st.selectbox(
            "Estrategia de prompts",
            options=["delimiters", "json", "fewshot", "cot"],
            format_func=lambda x: {
                "delimiters": "1 â€” Delimitadores",
                "json": "2 â€” Salida JSON",
                "fewshot": "3 â€” Few-Shot",
                "cot": "4 â€” Chain-of-Thought",
            }[x],
            key="strategy",
        )
        chunk_strategy = st.radio(
            "TamaÃ±o de chunks",
            options=["small", "large"],
            format_func=lambda x: "256 tokens" if x == "small" else "1024 tokens",
            horizontal=True,
            key="chunk_strategy",
        )
        top_k = st.slider("Top-k fragmentos", 1, 10, 5, key="top_k")

        st.divider()
        masked = "sk-â€¦" + st.session_state.api_key[-4:] if len(st.session_state.api_key) > 6 else "â€”"
        st.caption(f"ğŸ”‘ API Key activa: `{masked}`")
        if st.button("ğŸ”’ Cerrar sesiÃ³n", use_container_width=True):
            for k in ["api_key", "api_key_validated", "index_ready", "messages"]:
                st.session_state[k] = "" if k == "api_key" else (False if k != "messages" else [])
            st.rerun()

        st.divider()
        st.caption("Research Copilot v0.1.0")

    return strategy, chunk_strategy, top_k


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 1 â€” CHAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_chat_tab(strategy, chunk_strategy, top_k):
    st.header("ğŸ’¬ Chat con los Papers")
    st.caption(
        "Haz preguntas sobre crimen organizado, extorsiÃ³n y gobernanza criminal "
        "en AmÃ©rica Latina."
    )

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg.get("sources"):
                with st.expander("ğŸ“ Fuentes", expanded=False):
                    for s in msg["sources"]:
                        st.markdown(
                            f"**{s['title']}** â€” {', '.join(s['authors'][:2])} ({s['year']})  \n"
                            f"Relevancia: `{s['score']:.3f}` | {s.get('venue','')}"
                        )

    if prompt := st.chat_input("Â¿CuÃ¡l es tu pregunta de investigaciÃ³n?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            with st.spinner("Buscando en los papersâ€¦"):
                answer, sources = _run_rag(prompt, strategy, chunk_strategy, top_k)
            st.markdown(answer)
            if sources:
                with st.expander("ğŸ“ Fuentes", expanded=True):
                    for s in sources:
                        st.markdown(
                            f"**{s['title']}** â€” {', '.join(s['authors'][:2])} ({s['year']})  \n"
                            f"Relevancia: `{s['score']:.3f}` | {s.get('venue','')}"
                        )
        st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources})

    if st.session_state.messages:
        if st.button("ğŸ—‘ï¸ Limpiar conversaciÃ³n"):
            st.session_state.messages = []
            st.rerun()


def _run_rag(question, strategy, chunk_strategy, top_k):
    from src.retrieval import search
    from src.generation import generate_answer
    chroma = get_chroma_client_cached()
    oa = get_openai_client()
    try:
        chunks = search(question, top_k=top_k, strategy=chunk_strategy,
                        chroma_client=chroma, openai_client=oa)
    except RuntimeError as e:
        return f"âš ï¸ Error en la bÃºsqueda: {e}", []
    result = generate_answer(question=question, chunks=chunks, strategy=strategy, client=oa)
    sources = [{"title": c.title, "authors": c.authors, "year": c.year,
                "venue": c.venue, "score": c.score} for c in chunks]
    return result["answer"], sources


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 2 â€” PAPER BROWSER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_papers_tab():
    st.header("ğŸ“„ Explorador de Papers")
    papers = load_papers_metadata()
    if not papers:
        st.warning("No se encontrÃ³ papers.json.")
        return

    c1, c2, c3 = st.columns(3)
    with c1:
        years = sorted({p["year"] for p in papers if p.get("year")})
        sel_years = st.multiselect("AÃ±o", years)
    with c2:
        topics = sorted({t for p in papers for t in (p.get("topics") or [])})
        sel_topics = st.multiselect("Tema", topics)
    with c3:
        q = st.text_input("TÃ­tulo / autor", "").lower()

    filtered = papers
    if sel_years:
        filtered = [p for p in filtered if p.get("year") in sel_years]
    if sel_topics:
        filtered = [p for p in filtered if any(t in (p.get("topics") or []) for t in sel_topics)]
    if q:
        filtered = [p for p in filtered
                    if q in p.get("title","").lower()
                    or any(q in a.lower() for a in (p.get("authors") or []))]

    st.caption(f"Mostrando {len(filtered)} de {len(papers)} papers")
    for p in filtered:
        auths = "; ".join((p.get("authors") or [])[:3])
        if len(p.get("authors") or []) > 3:
            auths += " et al."
        with st.expander(f"**{p.get('title','?')}** â€” {auths} ({p.get('year','?')})"):
            ca, cb = st.columns([2, 1])
            with ca:
                st.markdown(f"**Autores:** {auths}")
                st.markdown(f"**Venue:** {p.get('venue') or 'â€”'}")
                if p.get("doi"):
                    st.markdown(f"**DOI:** [{p['doi']}](https://doi.org/{p['doi']})")
                if p.get("abstract"):
                    st.caption(p["abstract"])
            with cb:
                for t in (p.get("topics") or []):
                    st.markdown(f"- {t}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 3 â€” COMPARE STRATEGIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_compare_tab(chunk_strategy, top_k):
    st.header("ğŸ”¬ Comparar Estrategias de Prompts")
    question = st.text_area(
        "Pregunta:",
        value="Â¿CuÃ¡l es la relaciÃ³n entre gobernanza criminal y el Estado en AmÃ©rica Latina?",
        height=80, key="compare_q",
    )
    if st.button("â–¶ Ejecutar las 4 estrategias", type="primary"):
        from src.retrieval import search
        from src.generation import generate_answer, STRATEGY_LABELS
        chroma = get_chroma_client_cached()
        oa = get_openai_client()
        with st.spinner("Recuperando fragmentosâ€¦"):
            try:
                chunks = search(question, top_k=top_k, strategy=chunk_strategy,
                                chroma_client=chroma, openai_client=oa)
            except RuntimeError as e:
                st.error(str(e)); return
        st.success(f"{len(chunks)} fragmentos recuperados.")
        with st.expander("ğŸ“ Fragmentos", expanded=False):
            for c in chunks:
                st.markdown(f"**[{c.score:.3f}]** {c.title} ({c.year})")
                st.caption(c.text[:300] + "â€¦")
        cols = st.columns(2)
        for i, strat in enumerate(["delimiters", "json", "fewshot", "cot"]):
            with cols[i % 2]:
                st.subheader(STRATEGY_LABELS[strat])
                with st.spinner(f"Generandoâ€¦"):
                    res = generate_answer(question=question, chunks=chunks,
                                          strategy=strat, client=oa)
                st.markdown(res["answer"])
                st.caption(f"Tokens: {res['total_tokens']} | {res['elapsed_seconds']}s")
                st.divider()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 4 â€” ABOUT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_about_tab():
    st.header("â„¹ï¸ Acerca del Research Copilot")
    papers = load_papers_metadata()
    st.markdown("""
## Arquitectura RAG

```
20 PDFs â†’ PyMuPDF â†’ Chunking (256 / 1024 tok) â†’ text-embedding-3-small
                                                         â†“
                                                  ChromaDB (cosine)
                                                         â†“
Query â†’ embed â†’ Top-K Retrieval â†’ Prompt Strategy â†’ GPT-4o â†’ Respuesta
```

## Las 4 Estrategias de Prompting

| # | Estrategia | DescripciÃ³n |
|---|-----------|-------------|
| 1 | **Delimitadores** | Secciones `<<<CONTEXTO>>>` / `<<<PREGUNTA>>>` |
| 2 | **JSON Output** | Respuesta estructurada con campos predefinidos |
| 3 | **Few-Shot** | Dos ejemplos Q&A enseÃ±an el estilo esperado |
| 4 | **Chain-of-Thought** | 5 pasos explÃ­citos de razonamiento |

## Seguridad
La API key se solicita en el navegador y se guarda **solo en memoria de sesiÃ³n**.
Nunca se escribe en disco ni en el cÃ³digo fuente.

## Papers indexados
""")
    for p in (papers or []):
        auths = "; ".join((p.get("authors") or [])[:2])
        st.markdown(f"- **{p.get('title','?')}** â€” {auths} ({p.get('year','?')})")

    st.markdown("""
## Uso local
```bash
git clone https://github.com/krlos20203563/research-copilot
cd research-copilot
pip install -r requirements.txt
streamlit run app/streamlit_app.py
```
La app pedirÃ¡ la API key y construirÃ¡ el Ã­ndice automÃ¡ticamente en la primera ejecuciÃ³n.
""")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    # Step 1 â€” try .env for local dev
    _try_load_env_key()

    # Step 2 â€” API key gate
    if not render_api_key_gate():
        st.stop()

    # Step 3 â€” Index gate (auto-builds on first run)
    if not render_index_gate():
        st.stop()

    # Step 4 â€” Main app
    strategy, chunk_strategy, top_k = render_sidebar()
    tab_chat, tab_papers, tab_compare, tab_about = st.tabs([
        "ğŸ’¬ Chat", "ğŸ“„ Papers", "ğŸ”¬ Comparar", "â„¹ï¸ Acerca de",
    ])
    with tab_chat:    render_chat_tab(strategy, chunk_strategy, top_k)
    with tab_papers:  render_papers_tab()
    with tab_compare: render_compare_tab(chunk_strategy, top_k)
    with tab_about:   render_about_tab()


if __name__ == "__main__":
    main()
