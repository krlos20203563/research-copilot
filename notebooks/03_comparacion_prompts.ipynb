{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 03 — Comparación de Estrategias de Prompts\nComparación de las 4 estrategias de prompting en el sistema RAG."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, json\nfrom pathlib import Path\nROOT = Path(\"..\").resolve()\nsys.path.insert(0, str(ROOT))\nfrom dotenv import load_dotenv\nload_dotenv(ROOT / \".env\")\nprint(\"Setup OK\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.retrieval import search\nQUESTION = \"¿Cuáles son las principales estrategias de resistencia a la extorsión en América Latina?\"\nprint(f\"Question: {QUESTION}\n\")\nchunks = search(QUESTION, top_k=5, strategy=\"small\")\nprint(f\"Retrieved {len(chunks)} chunks:\")\nfor c in chunks:\n    print(f\"  [{c.score:.3f}] {c.title[:60]} ({c.year})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.generation import compare_strategies\nresults = compare_strategies(QUESTION, chunks)\nfor strat, res in results.items():\n    print(f\"\n{'='*60}\")\n    print(f\"STRATEGY: {res['strategy_label']}\")\n    print(f\"Tokens: {res['total_tokens']} | Time: {res['elapsed_seconds']}s\")\n    print('-'*60)\n    print(res['answer'][:800])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nstrategies = list(results.keys())\ntoken_counts = [results[s]['total_tokens'] for s in strategies]\nanswer_lens = [len(results[s]['answer']) for s in strategies]\nx = np.arange(len(strategies))\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nax1.bar(x, token_counts, color=['#2196F3','#4CAF50','#FF9800','#9C27B0'])\nax1.set_xticks(x); ax1.set_xticklabels(strategies, rotation=15)\nax1.set_title('Total Tokens Used'); ax1.set_ylabel('Tokens')\nax2.bar(x, answer_lens, color=['#2196F3','#4CAF50','#FF9800','#9C27B0'])\nax2.set_xticks(x); ax2.set_xticklabels(strategies, rotation=15)\nax2.set_title('Answer Length (chars)'); ax2.set_ylabel('Characters')\nplt.suptitle('Strategy Comparison Metrics')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "with open(ROOT / \"eval\" / \"test_questions.json\", encoding=\"utf-8\") as f:\n    eval_data = json.load(f)\nquestions_df_data = [{\"id\": q[\"id\"], \"category\": q[\"category\"], \"difficulty\": q[\"difficulty\"], \"question\": q[\"question\"][:80]} for q in eval_data[\"questions\"]]\nimport pandas as pd\ndf_q = pd.DataFrame(questions_df_data)\nprint(f\"Total test questions: {len(df_q)}\")\nprint(f\"\nBy category:\n{df_q['category'].value_counts()}\")\nprint(f\"\nBy difficulty:\n{df_q['difficulty'].value_counts()}\")\ndf_q.head(10)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}